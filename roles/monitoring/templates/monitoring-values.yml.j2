# =============================================================================
# kube-prometheus-stack Helm values
# Tuned for small K8s cluster (1 master + 2 workers)
# =============================================================================

# --- Prometheus ---
prometheus:
  prometheusSpec:
    scrapeInterval: "30s"
    evaluationInterval: "30s"
    retention: "{{ prometheus_retention }}"
    resources:
      requests:
        cpu: "250m"
        memory: "512Mi"
      limits:
        cpu: "1000m"
        memory: "{{ prometheus_memory_limit }}"
    # No persistent storage provisioner available â€” use emptyDir
    # Data survives pod restarts but not pod deletion
    storageSpec:
      emptyDir:
        medium: ""
        sizeLimit: "{{ prometheus_storage_size }}"

# --- Grafana ---
grafana:
  adminPassword: "{{ grafana_admin_password }}"
  service:
    type: NodePort
    nodePort: 30300
  resources:
    requests:
      cpu: "100m"
      memory: "128Mi"
    limits:
      cpu: "500m"
      memory: "{{ grafana_memory_limit }}"
  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
      labelValue: "1"
      searchNamespace: ALL

# --- AlertManager ---
alertmanager:
  alertmanagerSpec:
    resources:
      requests:
        cpu: "50m"
        memory: "64Mi"
      limits:
        cpu: "200m"
        memory: "{{ alertmanager_memory_limit }}"
  config:
    global:
      smtp_smarthost: "{{ alertmanager_smtp_host }}"
      smtp_from: "{{ alertmanager_smtp_from }}"
      smtp_auth_username: "{{ alertmanager_smtp_auth_username }}"
      smtp_auth_password: "{{ alertmanager_smtp_auth_password }}"
      smtp_require_tls: true
    route:
      group_by: ['alertname', 'namespace']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h
      receiver: 'email'
      routes:
        - match:
            severity: critical
          receiver: 'email'
          repeat_interval: 1h
        - match:
            severity: warning
          receiver: 'email'
          repeat_interval: 4h
    receivers:
      - name: 'email'
        email_configs:
          - to: "{{ alertmanager_email_to }}"
            send_resolved: true
            headers:
              Subject: '{% raw %}[K8s] {{ .Status | toUpper }} - {{ .CommonLabels.alertname }}{% endraw %}'

# --- Node Exporter ---
nodeExporter:
  resources:
    requests:
      cpu: "20m"
      memory: "32Mi"
    limits:
      cpu: "100m"
      memory: "64Mi"

# --- Disable resource-heavy components not needed ---
kubeProxy:
  enabled: false
kubeEtcd:
  enabled: false
kubeControllerManager:
  enabled: false
kubeScheduler:
  enabled: false

# --- Custom Alert Rules ---
additionalPrometheusRulesMap:
  coturn-alerts:
    groups:
      - name: coturn
        rules:
          - alert: COTURNPodDown
            expr: kube_deployment_status_replicas_available{namespace="coturn", deployment=~"coturn-worker.*"} < 1
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "COTURN pod {{ '{{' }} $labels.deployment {{ '}}' }} is down"
              description: "COTURN deployment {{ '{{' }} $labels.deployment {{ '}}' }} has 0 available replicas for more than 2 minutes."

          - alert: COTURNPodRestarting
            expr: increase(kube_pod_container_status_restarts_total{namespace="coturn"}[1h]) > 3
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: "COTURN pod {{ '{{' }} $labels.pod {{ '}}' }} is restarting frequently"
              description: "COTURN pod {{ '{{' }} $labels.pod {{ '}}' }} has restarted {{ '{{' }} $value {{ '}}' }} times in the last hour."

          - alert: COTURNHighMemory
            expr: >
              container_memory_working_set_bytes{namespace="coturn", container="coturn"}
              / on(pod) group_left() kube_pod_container_resource_limits{namespace="coturn", container="coturn", resource="memory"}
              > 0.85
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "COTURN pod {{ '{{' }} $labels.pod {{ '}}' }} high memory usage"
              description: "COTURN pod {{ '{{' }} $labels.pod {{ '}}' }} is using more than 85% of its memory limit."

      - name: node-health
        rules:
          - alert: NodeHighCPU
            expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Node {{ '{{' }} $labels.instance {{ '}}' }} high CPU usage"
              description: "Node {{ '{{' }} $labels.instance {{ '}}' }} CPU usage is above 85% for more than 10 minutes."

          - alert: NodeHighMemory
            expr: (1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 > 90
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Node {{ '{{' }} $labels.instance {{ '}}' }} high memory usage"
              description: "Node {{ '{{' }} $labels.instance {{ '}}' }} memory usage is above 90% for more than 5 minutes."

          - alert: NodeDiskSpaceLow
            expr: (1 - node_filesystem_avail_bytes{fstype!~"tmpfs|overlay"} / node_filesystem_size_bytes{fstype!~"tmpfs|overlay"}) * 100 > 85
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Node {{ '{{' }} $labels.instance {{ '}}' }} disk space low"
              description: "Node {{ '{{' }} $labels.instance {{ '}}' }} disk usage is above 85% on {{ '{{' }} $labels.mountpoint {{ '}}' }}."
